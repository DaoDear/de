# Lab 1: Fast Data Import with Sqoop

- [Using SQOOP to import data from MySQL to HDFS](#using-sqoop-to-import-data-from-mysql-to-hdfs)
- [Using SQOOP to import data from HDFS to MySQL](#using-sqoop-to-import-data-from-hdfs-to-mysql)

NOTE: MySQL is **case-sensitive**.  Be careful when use copy-and-paste.  
"$" is a prompt sign, meaning one line of command in Linux.  

## Using SQOOP to import data from MySQL to HDFS  

#### 1. Start MySQL service
```
$ sudo service mysqld start
```
#### 2. Login into MySQL, Create a database, then import existing tables into the database. Review the result
```
$ cd ~/lab1
$ mysql --user=root --password=cloudera < sqoop_db.sql
$ mysql --user=root --password=cloudera sqoop_db
mysql> SHOW TABLES;
mysql> SELECT * FROM transaction LIMIT 10;
mysql> quit;
```
#### 3. List database by using SQOOP command
```
$ sqoop list-databases --connect jdbc:mysql://localhost:3306 --username root --password cloudera
```
#### 4. List tables by using SQOOP command
```
$ sqoop list-tables --connect jdbc:mysql://localhost:3306/sqoop_db --username root --password cloudera
```
#### 5. Import data from table 'transaction' to HDFS using SQOOP Command
```
$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop_db --table transaction --target-dir /user/cloudera/transaction --username root --password cloudera
```
Investigate result
```
$ hdfs dfs -cat /user/cloudera/transaction/* | head -100
```
#### 6. Update database and import the data to HDFS
```
$ mysql --user=root --password=cloudera sqoop_db
mysql> CREATE table data (id INT PRIMARY KEY, data VARCHAR(30), time TIMESTAMP) ENGINE=InnoDB DEFAULT CHARSET=utf8;
mysql> INSERT into data values(1, '100', now());
mysql> INSERT into data values(2, '200', now());
mysql> INSERT into data values(3, '300', now());
mysql> select * from data;
mysql> quit;
```
Import database
```
$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop_db --table data --target-dir /user/cloudera/data --username root --password cloudera --driver 'com.mysql.jdbc.Driver'
```
Investigate result
```
$ hdfs dfs -cat data/*
```
Update database
```
$ mysql --user=root --password=cloudera sqoop_db
mysql> UPDATE data SET data= '200' where id=1;
mysql> select * from data;
mysql> quit;
```
Re-import database with incremental flag
```
$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop_db --table data --target-dir /user/cloudera/data --username root --password cloudera --driver 'com.mysql.jdbc.Driver' --incremental lastmodified --check-column time --merge-key id
```
Investigate result
```
$ hdfs dfs -cat data/*
```
#### 7. Data insertion and incremental import the data to HDFS
```
$ mysql --user=root --password=cloudera sqoop_db
mysql> INSERT into data VALUES (4, 300, now()) , (5, 400, now()), (6,500, now());
mysql> SELECT * FROM data;
mysql> quit;
```
Re-import database with incremental flag
```
$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop_db --table data --target-dir /user/cloudera/data --username root --password cloudera --driver 'com.mysql.jdbc.Driver' --incremental append --check-column id --last-value 3
```
Investigate result
```
$ hdfs dfs -cat data/*
```
#### 8. Query with where condition
Re-import database with where condition flag
```
$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop_db --table data --where "id>3" --target-dir /user/cloudera/data2 --username root --password cloudera --driver 'com.mysql.jdbc.Driver'
```
Investigate result
```
$ hdfs dfs -cat data2/*
```
#### 9. Free-form query imports
Re-import database with free form query flag
```
$ sqoop import --connect jdbc:mysql://localhost:3306/sqoop_db --target-dir /user/cloudera/data3 --username root --password cloudera --driver 'com.mysql.jdbc.Driver' --query "select id, data from data where id>4 and \$CONDITIONS" --split-by id
```
Investigate result
```
$ hdfs dfs -cat data3/*
```

## Using SQOOP to import data from HDFS to MySQL
NOTE: Require data from the previous task.

#### 1)	Export whole data form HDFS to MySQL
Create new table on MySQL
```
$ mysql --user=root --password=cloudera sqoop_db
mysql> SHOW TABLES;
mysql> CREATE TABLE transaction_import LIKE transaction;
mysql> SELECT COUNT(*) FROM transaction_import;
mysql> quit;
```
Export data from HDFS directory with SQOOP
```
$ sqoop export --connect jdbc:mysql://localhost:3306/sqoop_db --table transaction_import --export-dir /user/cloudera/transaction --username root --password cloudera
```
Investigate result
```
$ mysql --user=root --password=cloudera sqoop_db
mysql> SHOW TABLES;
mysql> SELECT COUNT(*) FROM transaction_import;
mysql> SELECT * FROM transaction_import LIMIT 100;
mysql> quit;
```
#### 2)	Export the data from HDFS to MySQL
```
$ mysql --user=root --password=cloudera sqoop_db
mysql> CREATE table emp (id INT NOT NULL PRIMARY KEY, name VARCHAR(50));
mysql> quit;

$ cat emp1.txt
$ cat emp2.txt 
$ hdfs  dfs -mkdir emp1/
$ hdfs dfs -put emp1.txt emp1/
$ hdfs dfs -cat emp1/emp1.txt
$ sqoop export --connect jdbc:mysql://localhost/sqoop_db --username root --password  cloudera --table emp --export-dir emp1/ --input-fields-terminated-by ','

$ mysql --user=root --password=cloudera sqoop_db
mysql> select * from emp;
mysql> quit;
```
#### 3)	Update the data and re-export from HDFS to MySQL
```
$ hdfs dfs -mkdir emp2/
$ hdfs dfs -put emp2.txt emp2/
$ hdfs dfs -cat emp2/emp2.txt
$ sqoop export --connect jdbc:mysql://localhost/sqoop_db --username root --password cloudera --table emp --export-dir emp2/ --update-mode allowinsert --update-key id --input-fields-terminated-by ','

$ mysql --user=root --password=cloudera sqoop_db
mysql> select * from emp;
mysql> quit;
```
