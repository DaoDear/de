# Lab 2: Get Log Data with Flume

Dataset Explanations: Multiple CSV format files are on a local path. The data in the files is banking transactions.

## Upload multiple files to HDFS using FLUME

#### 1) Review file content and prepare a directory on HDFS
Investigate files on local directory
```
$ cd ~/lab2
$ head -n 10 flume_data/001.csv
$ head -n 10 flume_data/002.csv
$ head -n 10 flume_data/003.csv
```
Create output directory on HDFS
```
$ hdfs dfs -mkdir /user/cloudera/flume_output
```
#### 2) Create a FLUME configuration file to spool a local directory
Investigate files on local directory
```
$ cd ~/lab2
$ gedit flume.conf &
```
#### 3) Flume Configuration File (flume.conf)
```
cloudera_agent.sources = src1
cloudera_agent.channels = ch1
cloudera_agent.sinks = sink1

cloudera_agent.sources.src1.type = spooldir
cloudera_agent.sources.src1.channels = ch1
cloudera_agent.sources.src1.spoolDir = /home/cloudera/lab2/flume_data
cloudera_agent.sources.src1.batchSize = 1000

cloudera_agent.channels.ch1.type = memory
cloudera_agent.channels.ch1.capacity = 100000000
cloudera_agent.channels.ch1.transactionCapacity = 5000

cloudera_agent.sinks.sink1.type = hdfs
cloudera_agent.sinks.sink1.channel = ch1
cloudera_agent.sinks.sink1.hdfs.batchSize = 5000
cloudera_agent.sinks.sink1.hdfs.path = /user/cloudera/flume_output
cloudera_agent.sinks.sink1.hdfs.filePrefix = log-output
cloudera_agent.sinks.sink1.hdfs.rollSize = 5120000
cloudera_agent.sinks.sink1.hdfs.rollInterval = 10
cloudera_agent.sinks.sink1.hdfs.rollCount = 0
cloudera_agent.sinks.sink1.hdfs.fileType = DataStream
cloudera_agent.sinks.sink1.hdfs.writeFormat = Text
```
#### 4)	Start a FLUME agent to ingest files into HDFS
```
$ flume-ng agent -n cloudera_agent -f flume.conf -Xms1024m -Xmx2048m
```
NOTE: -Xms and â€“Xmx is memory configurable for Flume Java Heap
#### 5) View the result on HDFS
View the result on HDFS  
```
[Open new terminal]
$ hdfs dfs -cat flume_output/*
```
